# E-commerce Image Validator

An intelligent system for assessing the quality of product images for e-commerce use. Combines computer vision feature extraction with multiple LLM reasoners to provide explainable quality assessments.

## ğŸ¯ Features

- **Multi-Feature Extraction:**
  - Blur/sharpness detection (Laplacian variance)
  - Object detection (YOLOv8)
  - Background quality analysis (custom heuristics)

- **Multi-Model AI Reasoning:**
  - ğŸ¦™ **Llama 3.3 70B** (via Groq) - Fast, free, excellent reasoning
  - ğŸ§  **Claude 3.5 Sonnet** (via AWS Bedrock) - Premium reasoning, structured output
  - âœ¨ **Gemini 2.5 Flash** (via Google) - free tier available
  - Compare outputs across all models

- **Evaluation & Analysis:**
  - Ground truth evaluation with metrics (accuracy, precision, recall, F1)
  - Multi-model comparison script
  - Confidence scoring and uncertainty handling

- **Interactive UI:**
  - Streamlit web interface
  - Model selection dropdown
  - Multi-model comparison mode
  - Real-time analysis
  - JSON export functionality

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- UV package manager ([install](https://docs.astral.sh/uv/))
- At least one of the following API keys:
  - Groq API key (free tier at [groq.com](https://groq.com))
  - AWS credentials with Bedrock access (for Claude)
  - Google API key (free tier at [ai.google.dev](https://ai.google.dev))

### Installation

1. **Clone the repository:**
```bash
git clone https://github.com/KRT2002/ecommerce-image-validator.git
cd ecommerce-image-validator
```

2. **Install dependencies with UV:**
```bash
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install -e .
```

3. **Set up environment variables:**
```bash
cp .env.example .env
# Edit .env and add your API keys (at minimum GROQ_API_KEY)
```

**Minimum .env setup (Groq only):**
```bash
GROQ_API_KEY=your_groq_api_key_here
MODEL_NAME=llama-3.3-70b-versatile
TEMPERATURE=0.1
```

**Full .env setup (all models):**
```bash
# Groq
GROQ_API_KEY=your_groq_key

# AWS Bedrock (for Claude)
AWS_ACCESS_KEY_ID=your_aws_key
AWS_SECRET_ACCESS_KEY=your_aws_secret
AWS_REGION=us-east-1
CLAUDE_MODEL_ID=anthropic.claude-3-5-sonnet-20241022-v2:0

# Google (for Gemini)
GOOGLE_API_KEY=your_google_key
GEMINI_MODEL_ID=gemini-2.0-flash-exp
```

4. **Run the Streamlit app:**
```bash
streamlit run app.py
```

The app will open in your browser at `http://localhost:8501`

## ğŸ“ Project Structure

```
ecommerce-image-validator/
â”œâ”€â”€ src/validator/
â”‚   â”œâ”€â”€ extractors/          # Feature extraction modules
â”‚   â”‚   â”œâ”€â”€ base.py          # Base extractor class
â”‚   â”‚   â”œâ”€â”€ blur_detector.py # Sharpness detection
â”‚   â”‚   â”œâ”€â”€ object_detector.py # YOLOv8 wrapper
â”‚   â”‚   â””â”€â”€ background_analyzer.py # Background quality
â”‚   â”œâ”€â”€ llm/                 # LLM reasoning
â”‚   â”‚   â”œâ”€â”€ base.py          # Base LLM class
â”‚   â”‚   â”œâ”€â”€ groq_reasoner.py # Llama 3.3 via Groq
â”‚   â”‚   â”œâ”€â”€ claude_reasoner.py # Claude via AWS Bedrock
â”‚   â”‚   â”œâ”€â”€ gemini_reasoner.py # Gemini via Google
â”‚   â”‚   â””â”€â”€ prompts.py       # Prompt templates
â”‚   â”œâ”€â”€ config.py            # Configuration
â”‚   â”œâ”€â”€ logger.py            # Logging setup
â”‚   â”œâ”€â”€ utils.py             # Utility functions
â”‚   â””â”€â”€ pipeline.py          # Main orchestrator
â”œâ”€â”€ scripts/                 # Utility scripts
â”‚   â”œâ”€â”€ compare_models.py    # Multi-model comparison
â”‚   â””â”€â”€ evaluate.py          # Evaluation with metrics
â”œâ”€â”€ app.py                   # Streamlit frontend
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ images/              # Sample images
â”‚   â”œâ”€â”€ evaluation/          # Test dataset
â”‚   â”‚   â”œâ”€â”€ good/            # Suitable images (ground truth)
â”‚   â”‚   â””â”€â”€ bad/             # Unsuitable images (ground truth)
â”‚   â””â”€â”€ outputs/
â”‚       â””â”€â”€ comparison_results/ # Saved comparison JSONs
â”œâ”€â”€ pyproject.toml           # UV configuration
â””â”€â”€ README.md
```

## ğŸ”§ Usage

### 1. Via Streamlit UI (Recommended)

```bash
streamlit run app.py
```

**Features:**
- Upload product images
- Select which LLM model to use (Groq/Claude/Gemini)
- Enable "Compare All Models" to run all 3 LLMs
- View detailed analysis and export results

### 2. Programmatic Usage

**Single model:**
```python
from validator import ImageValidationPipeline

# Initialize with specific model
pipeline = ImageValidationPipeline(llm_type="groq")  # or "claude" or "gemini"

# Validate an image
result = pipeline.validate("path/to/product.jpg")

# Access results
print(f"Verdict: {result.verdict}")
print(f"Quality Score: {result.quality_score:.2f}")
print(f"Reasoning: {result.reasoning}")
print(f"Issues: {result.issues_detected}")
```

### 3. Multi-Model Comparison Script

Compare all models on a single image:

```bash
# Compare all models (default)
python scripts/compare_models.py --image examples/images/product.jpg

# Compare specific models only
python scripts/compare_models.py --image product.jpg --models groq,claude

# Save results and show detailed reasoning
python scripts/compare_models.py --image product.jpg --output results.json --detailed
```

**Example output:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•—
â•‘ Model            â•‘ Verdict       â•‘ Score â•‘ Confidence â•‘ Time  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•£
â•‘ GROQ             â•‘ âœ… suitable   â•‘ 0.78  â•‘ 0.85       â•‘ 2.34s â•‘
â•‘ CLAUDE           â•‘ âœ… suitable   â•‘ 0.82  â•‘ 0.90       â•‘ 3.12s â•‘
â•‘ GEMINI           â•‘ âš ï¸  uncertain â•‘ 0.65  â•‘ 0.70       â•‘ 1.89s â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•

âœ… AGREEMENT: All models agree!
   Consensus verdict: suitable
```

### 4. Evaluation Script

Evaluate system accuracy on labeled dataset:

**Setup:**
```bash
# Create evaluation dataset
mkdir -p examples/evaluation/good examples/evaluation/bad

# Add images:
# - examples/evaluation/good/ â†’ suitable product images
# - examples/evaluation/bad/ â†’ unsuitable product images
```

**Run evaluation:**
```bash
# Evaluate single model
python scripts/evaluate.py --model groq

# Compare all models
python scripts/evaluate.py --all-models

# Custom evaluation directory
python scripts/evaluate.py --eval-dir my_dataset/ --model claude
```

**Example output:**
```
================================================================================
EVALUATION RESULTS: GROQ
================================================================================

Total Images: 20
Accuracy: 85.0% (17/20)
Precision: 88.9%
Recall: 80.0%
F1-Score: 84.2%

CONFUSION MATRIX
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      â”‚ Predicted Suitable â”‚ Predicted Not Suitable   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Actually Suitable    â”‚ 8                  â”‚ 2                        â”‚
â”‚ Actually Not Suitableâ”‚ 1                  â”‚ 9                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Example Output (JSON)

```json
{
  "image_path": "product.jpg",
  "quality_score": 0.78,
  "verdict": "suitable",
  "reasoning": "The image demonstrates good sharpness with a Laplacian variance of 245.3...",
  "issues_detected": [],
  "confidence": 0.85,
  "extracted_features": {
    "blur_detection": {
      "variance": 245.3,
      "is_sharp": true,
      "sharpness_score": 0.82
    },
    "object_detection": {
      "objects": [{"class": "shoe", "confidence": 0.94}],
      "num_objects": 1,
      "primary_object": {"class": "shoe", "confidence": 0.94}
    },
    "background_analysis": {
      "cleanliness_score": 0.76,
      "is_clean": true,
      "edge_density": 0.15,
      "color_variance": 0.22
    }
  },
  "feature_importance": {
    "sharpness": 0.35,
    "background": 0.40,
    "objects": 0.25
  },
  "metadata": {
    "processing_time_seconds": 2.34,
    "llm_model": "llama-3.3-70b-versatile"
  }
}
```

## âš™ï¸ Configuration

### Model Selection

Edit `.env` to choose default model:
```bash
DEFAULT_MODEL=groq  # Options: groq, claude, gemini
```

### Feature Extraction Thresholds

Adjust sensitivity in `.env`:
```bash
BLUR_THRESHOLD=100.0                      # Lower = more strict on blur
BACKGROUND_CLEANLINESS_THRESHOLD=0.6      # Higher = cleaner background required
MIN_OBJECT_CONFIDENCE=0.5                 # Confidence threshold for object detection
```

### Logging

```bash
LOG_LEVEL=INFO  # Options: DEBUG, INFO, WARNING, ERROR
```

## ğŸ“ Technical Write-up

See [technical_writeup.md](technical_writeup.md) for:
- System architecture details
- Model comparison and trade-offs
- Design decisions and justifications
- Limitations and failure modes
- Production deployment considerations
- Evaluation methodology

## ğŸ” Model Comparison

| Model | Provider | Speed | Cost | Reasoning Quality | Best For |
|-------|----------|-------|------|-------------------|----------|
| **Llama 3.3 70B** | Groq | âš¡âš¡âš¡ Fast (2-3s) | Free | â­â­â­â­ Excellent | Development, fast iteration |
| **Claude 3.5 Sonnet** | AWS Bedrock | âš¡âš¡ Medium (5-7s) | $$$ Paid | â­â­â­â­â­ Best | Production, critical decisions |
| **Gemini 2.5 Flash** | Google | âš¡âš¡ Medium (5-7s) | Free tier | â­â­â­â­ Very Good | Large-context tasks, structured reasoning |

**Recommendation:** 
- **Development:** Use Groq (free, fast, good quality)
- **Production:** Use Claude for best accuracy, Gemini for speed
- **Validation:** Run all 3 and use consensus voting

## ğŸš§ Limitations

- **Blur detection:** Can be fooled by intentional bokeh or high-contrast edges
- **Object detection:** Limited to 80 COCO classes; may miss niche products
- **Background analysis:** Cannot distinguish foreground from background without segmentation
- **Cultural bias:** "Professional" aesthetics may vary across cultures
- **LLM hallucinations:** Models may occasionally invent features not present
- **Model disagreement:** Different LLMs can give conflicting verdicts

## ğŸ”® Future Improvements

- [ ] Add semantic segmentation for precise background isolation
- [ ] Implement OCR for brand/label text extraction
- [ ] Add CLIP embeddings for semantic "professionalism" scoring
- [ ] Support batch processing of multiple images
- [ ] Add result caching for duplicate images
- [ ] Implement ensemble voting for multi-model consensus
- [ ] Create larger evaluation dataset with domain expert labels
- [ ] Add confidence calibration and uncertainty quantification
- [ ] Support custom fine-tuning of quality thresholds per use case

## ğŸ’¡ Design Decisions & Trade-offs

### Why Multiple LLMs?

**Benefits:**
- Reduces single-model bias
- Provides confidence through consensus
- Allows fallback if one API fails
- Enables cost/speed/quality trade-offs

**Trade-offs:**
- More complex setup (multiple API keys)
- Higher latency when comparing all models
- Potential disagreements require handling

### Why These Specific Models?

**Llama 3.3 70B (Groq):**
- âœ… Free and fast via Groq infrastructure
- âœ… Excellent reasoning for this task
- âŒ Requires Groq account

**Claude 3.5 Sonnet (AWS Bedrock):**
- âœ… Best-in-class reasoning and structured output
- âœ… Enterprise-grade reliability
- âŒ Costs money, requires AWS setup

**Gemini 2.5 Flash (Google):**
- âœ… Strong reasoning and structured outputs
- âœ… Generous free tier
- âœ… Good at structured tasks
- âŒ Slightly higher latency than ultra-light models

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) file for details

## ğŸ¤ Contributing

Contributions welcome! Please open an issue or PR.

## ğŸ™ Acknowledgments

- YOLOv8 by Ultralytics
- Groq for fast LLM inference
- AWS Bedrock for Claude access
- Google for Gemini API
- OpenCV community
- Streamlit team
- LangChain for LLM wrappers

## ğŸ“š Additional Resources

- [Technical Write-up](technical_writeup.md) - Detailed architecture and decisions
- [API Documentation](https://docs.claude.com) - Claude API docs
- [Groq Documentation](https://console.groq.com/docs) - Groq API docs
- [Gemini Documentation](https://ai.google.dev/docs) - Gemini API docs
- [YOLOv8 Documentation](https://docs.ultralytics.com) - Object detection

## ğŸ†˜ Troubleshooting

**Issue:** "Missing GROQ_API_KEY"
- **Solution:** Add `GROQ_API_KEY=your_key` to `.env` file

**Issue:** Claude reasoner fails
- **Solution:** Verify AWS credentials and Bedrock access in your region

**Issue:** Gemini rate limit exceeded
- **Solution:** Check your Google API quota, wait, or upgrade tier

**Issue:** YOLO model download fails
- **Solution:** Check internet connection, model will auto-download (~6MB)

**Issue:** Models disagree on verdict
- **Solution:** This is normal! Use the comparison output to make informed decisions

---

For detailed technical documentation, see [technical_writeup.md](technical_writeup.md)